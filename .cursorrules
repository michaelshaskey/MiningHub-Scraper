# MiningHub Data Processor - Cursor Rules

## Project Overview
Cloud-native, modular data processing system for MiningHub project data, built following 12/15-factor app principles with proper dependency management and deployment optimization.

## Key Files & Commands
- **Main Application**: `python3 app.py` (12/15-factor compliant entry point)
- **Primary Outputs**: 
  - `outputs/json_outputs/companies_with_projects_*.json` (CRM-ready data)
  - `outputs/excel_outputs/mining_projects_*.xlsx` (Excel reports)
- **Configuration**: Environment variables via `.env` file (create from `envexample.txt`)

## Critical Information
- **JWT Token**: Set via `JWT_TOKEN` environment variable (secure)
- **Architecture**: Modular design with `core/`, `services/`, and `scripts/` separation
- **Processing**: API → Relationships → Scraper fallback → Geocoding → Storage
- **Resource Usage**: Configurable batch size (default 100), 4 parallel workers
- **Rate Limiting**: Built-in API rate limiting and retry logic

## Current Architecture

### Core Files (Required - 10 files total)
- ✅ `app.py` - Main 12/15-factor application entry point
- ✅ `core/__init__.py` - Core module initialization
- ✅ `core/assembly.py` - Project assembly and company resolution
- ✅ `core/discovery.py` - GID discovery from API and URLs
- ✅ `core/storage.py` - Data persistence and export
- ✅ `services/__init__.py` - Services module initialization
- ✅ `services/api_client.py` - MiningHub API client with retry logic
- ✅ `services/geocoding.py` - Location enrichment service
- ✅ `services/map_center.py` - Map coordinate extraction (7s timeout)
- ✅ `services/playwright_parallel_scraper.py` - Parallel web scraping

### Support Files
- ✅ `countries.json` - List of 198 countries to process
- ✅ `found_urls.xlsx` - Project and company URL mappings
- ✅ `requirements.txt` - Python dependencies
- ✅ `simple_dependency_tracer.py` - Project dependency analysis tool

### Cleaned Up (Removed)
- ❌ `Old Scripts/` - Legacy monolithic scripts (5 files removed)
- ❌ Cache files - Geocoding cache (6.6MB) and old outputs
- ❌ Log files - Application logs not needed for deployment
- ❌ Analysis files - Temporary audit and dependency reports

## Quick Commands

### Production Run (All Countries)
```bash
# Full production pipeline
cd "/path/to/Data Cleanup"
PROCESSING_MODE=production RUN_MODE=pipeline python3 app.py
```

### Test Run (Limited Scope)
```bash
# Test with Australia only, 10 projects max
PROCESSING_MODE=test python3 app.py
```

### Individual Phases
```bash
python3 app.py discovery    # Find all project GIDs
python3 app.py assembly     # Process and assemble projects
python3 app.py health       # Application health check
```

### Utility Scripts
```bash
# Analyze project dependencies
python3 simple_dependency_tracer.py

# Environment setup
cp envexample.txt .env
# Edit .env with JWT_TOKEN
```

## Configuration Management

### Environment Variables (.env file)
```bash
# Required
JWT_TOKEN=your_jwt_token_here

# Processing Mode
PROCESSING_MODE=test|production  # test = limited scope, production = all countries
BATCH_SIZE=100                   # Projects per batch (optimized)
LOG_LEVEL=INFO                   # DEBUG|INFO|WARNING
LOG_TO_CONSOLE=true              # Also log to console (default: true)

# API Settings
API_BASE_URL=https://mininghub.com/api
API_TIMEOUT=30
API_RETRY_ATTEMPTS=3

# Feature Toggles
ENABLE_GEOCODING=true            # Location enrichment
SCRAPER_HEADFUL=false           # Set true for debugging scraper

# Output
OUTPUT_DIR=outputs
```

### Mode-Specific Behavior
- **Test Mode**: Single country (Australia), max 10 projects, fast feedback
- **Production Mode**: All 198 countries, unlimited projects, full processing

## Data Flow Pipeline

### Phase 1: Discovery
1. **Load Countries** - From `countries.json` (198 countries)
2. **API Discovery** - Call `/projects/filter` for each country
3. **URL Discovery** - Extract GIDs from `found_urls.xlsx`
4. **Deduplication** - Unique GID set for processing

### Phase 2: Assembly (Batch Processing)
1. **Project Data** - Extract safe project fields from API cache
2. **Company Resolution** - Call `/project/relationships` for authoritative data
3. **Scraper Fallback** - Playwright scraping if relationships fail
4. **Map Center Fetch** - 7s timeout for coordinate extraction
5. **Geocoding** - Reverse geocode coordinates to addresses

### Phase 3: Storage & Export
1. **JSON Export** - `companies_with_projects_*.json` (timestamped)
2. **Excel Export** - Multi-sheet Excel with projects, companies, relationships
3. **Metrics** - Processing statistics and performance data

## Performance & Resource Management

### Optimized Settings
- **Batch Size**: 100 projects (increased from 50)
- **Map Timeout**: 7 seconds (increased from 4s)
- **Parallel Workers**: 4 Chrome instances for scraping
- **Memory Usage**: ~600MB per worker + 2GB base system

### Rate Limiting
- **API Calls**: Built-in retry logic with exponential backoff
- **Geocoding**: 1-second delay between requests (Nominatim)
- **Web Scraping**: 0.2-second delay per worker

### Resource Monitoring
```bash
# Check Chrome processes
ps aux | grep chrome | wc -l

# Monitor memory usage
top -o MEM

# Check output sizes
du -sh outputs/
```

## Deployment Optimization

### Size Reduction Achieved
- **Before**: 112MB with cache and old outputs
- **After**: 47MB (58% reduction)
- **Core Application**: Only 10 files needed (43.5% of codebase)

### Cloud Deployment Ready
- ✅ **Environment-based config**: No hardcoded secrets
- ✅ **Stateless processes**: Horizontal scaling compatible
- ✅ **Structured logging**: JSON format for log aggregation
- ✅ **Health checks**: Built-in health endpoint
- ✅ **Graceful shutdown**: SIGTERM/SIGINT handling

## Troubleshooting

### Common Issues
1. **"No cached data found for GID"** - Ensure `PROCESSING_MODE=production` for global preload
2. **Playwright timeouts** - Increase map timeout or reduce concurrent workers
3. **JWT token expired** - Update `JWT_TOKEN` in `.env` file
4. **Memory issues** - Reduce batch size or scraper workers

### Debug Commands
```bash
# Enable debug logging
LOG_LEVEL=DEBUG python3 app.py

# Run scraper in visible mode (debugging)
SCRAPER_HEADFUL=true python3 app.py

# Check dependency usage
python3 simple_dependency_tracer.py
```

## Success Metrics
- **API Coverage**: >95% of projects have API data
- **Scraping Success**: >90% success rate for fallback scraping  
- **Company Resolution**: >80% via relationships endpoint
- **Processing Speed**: ~0.04-0.16 projects/sec per worker

## Important Notes
- Always use `python3` instead of `python`
- Ensure `.env` file exists with `JWT_TOKEN` before running
- Monitor system resources during large production runs
- Cache files regenerate automatically - safe to delete
- Output files are timestamped - clean up old ones periodically

## File Lifecycle
- **Inputs**: `countries.json`, `found_urls.xlsx` (required)
- **Outputs**: Timestamped JSON/Excel files in `outputs/`
- **Cache**: `outputs/cache/geocoding_cache.json` (auto-regenerated)
- **Logs**: Structured JSON to timestamped log files in `logs/` directory + optional console output

This system is now optimized for cloud deployment with proper dependency management, configuration via environment variables, and significant size reduction while maintaining full functionality.